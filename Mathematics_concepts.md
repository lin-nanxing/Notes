# 1.Kronecker积
- Kronecker积在张量计算中非常常见，是衔接矩阵计算和张量计算的重要桥梁。刚开始接触张量计算的读者可能会被Kronecker积的名称或是符号唬住，但实际上这是完全没有必要的，因为Kronecker积的运算规则是非常容易理解的。
![计算方式](/jpg/屏幕截图%202024-01-27%20112648.png)
![Alt text](/jpg/image.png)不一定相等
- **Kronecker积存在结合律和分配律，这两个性质是Kronecker积最为朴素的性质。**    
![两个性质](/jpg/屏幕截图%202024-01-27%20113205.png)
![Alt text](/jpg/image-1.png)
- 就前面已经提到的一些基本性质而言，它们要么是从Kronecker积本身的运算规则中衍生而来的，要么就与矩阵的一些基本运算直接相关，这些基本运算包括了转置、相乘以及求逆矩阵，然而，这些基本运算都不能用于描述矩阵的特征。下面，我们将着重分析Kronecker积得到的矩阵具有怎样的性质，在内容上，我们会介绍一些用于描述矩阵特征的“指标”，这些指标都来自于线性代数，例如，矩阵的迹表示矩阵主对角线元素之和、F-范数表示矩阵所有元素的平方和开根号。归纳来看，有以下五个性质：
![补充](/jpg/屏幕截图%202024-01-27%20113358.png)

# 2.奇异值分解（SVD）
- 奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SVD的。
- SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个m×n的矩阵，那么我们定义矩阵A的SVD为：
![1](/jpg/屏幕截图%202024-01-27%20125512.png)
![2](/jpg/屏幕截图%202024-01-27%20130122.png)
![3](/jpg/屏幕截图%202024-01-27%20130140.png)
![4](/jpg/屏幕截图%202024-01-27%20130342.png)
![5](/jpg/屏幕截图%202024-01-27%20130410.png)
![6](/jpg/屏幕截图%202024-01-27%20130429.png)
- 对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。
也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。
![1](/jpg/屏幕截图%202024-01-27%20130915.png)
- **SVD用于PCA**
![2](/jpg/屏幕截图%202024-01-27%20131044.png)
- **左奇异矩阵`U`可以用于行数的压缩。    
  右奇异矩阵`V`可以用于列数即特征维度的压缩，也就是我们的PCA降维。**
___
- SVD作为一个很基本的算法，在很多机器学习算法中都有它的身影，特别是在现在的大数据时代，由于SVD可以实现并行化，因此更是大展身手。
SVD的缺点是分解出的矩阵解释性往往不强，有点黑盒子的味道，不过这不影响它的使用。
- SVD在工程应用中的很多地方也有它的身影，比如PCA，掌握了SVD原理后再去看PCA那是相当简单的; 在推荐系统方面，SVD更是名声大噪，将它应用于推荐系统的是Netflix大奖的获得者Koren，可以在Google上找到他写的文章；用SVD可以很容易得到任意矩阵的满秩分解，用满秩分解可以对数据做压缩。可以用SVD来证明对任意M*N的矩阵均存在如下分解：
![3](/jpg/20200505195130763.png)

# 3.